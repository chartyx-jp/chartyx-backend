# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ
# from google.colab import drive
# drive.mount('/content/drive')


# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# !pip install pyarrow scikit-learn pandas  xgboost --quiet

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
class ParquetHandler:
    def __init__(self, directory: str):
        self.directory = directory
        os.makedirs(self.directory, exist_ok=True)

    def load_batch(self, files: list) -> pd.DataFrame:
        dfs = []
        for f in files:
            try:
                df = pd.read_parquet(os.path.join(self.directory, f))
                dfs.append(df)
            except Exception as e:
                print(f"âŒ èª­ã¿è¾¼ã¿å¤±æ•—: {f} - {e}")
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    @staticmethod
    def split(df: pd.DataFrame):
        df = df.copy()
        df = df.dropna(subset=["Target"])

        X = df.drop(columns=["Date", "Target"], errors="ignore")

        #  inf / -inf ã‚’ NaN ã«å¤‰æ›ã—ã¦ã‹ã‚‰ dropna
        X.replace([np.inf, -np.inf], np.nan, inplace=True)
        X.dropna(inplace=True)

        y = df.loc[X.index, "Target"]
        return X, y

class StockAITrainer:
    def __init__(self):
        self.booster = None
        self.params = {
            "objective": "reg:squarederror",
            "tree_method": "hist",
            "device": "cuda",  # CPUãªã‚‰å‰Šé™¤ã‹å¤‰æ›´
            "eval_metric": "mae",
        }

    def train_incrementally(self, X, y):
        dtrain = xgb.DMatrix(X, label=y)
        if self.booster is None:
            self.booster = xgb.train(self.params, dtrain, num_boost_round=100)
        else:
            self.booster = xgb.train(self.params, dtrain, num_boost_round=100, xgb_model=self.booster)

    def evaluate(self, X, y):
        dval = xgb.DMatrix(X)
        preds = self.booster.predict(dval)
        mae = mean_absolute_error(y, preds)
        print(f"ğŸ“Š ãƒãƒƒãƒè©•ä¾¡ MAE={mae:.4f}")
        return mae

    def save_model(self, path):
        self.booster.save_model(path)

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨
if __name__ == "__main__":
    PROCESSED_DIR = "/content/drive/MyDrive/chartyx-ai-colab/processed_data"
    MODEL_SAVE_PATH = "/content/drive/MyDrive/chartyx-ai-colab/models/chartyx_v2.json"
    BATCH_SIZE = 500

    print("ğŸš€ Parquetãƒãƒƒãƒã«ã‚ˆã‚‹AIè¨“ç·´ã‚’é–‹å§‹...")

    parquet_handler = ParquetHandler(PROCESSED_DIR)
    trainer = StockAITrainer()

    files = [f for f in os.listdir(PROCESSED_DIR) if f.endswith(".parquet")]
    total_files = len(files)
    total_rows_loaded = 0
    total_rows_used = 0
    total_batches = (total_files + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, total_files, BATCH_SIZE):
        batch_files = files[i:i + BATCH_SIZE]
        print(f"\nğŸ“‚ ãƒãƒƒãƒ {i//BATCH_SIZE+1}/{total_batches}: {len(batch_files)}ä»¶èª­ã¿è¾¼ã¿ä¸­...")

        df_batch = parquet_handler.load_batch(batch_files)
        if df_batch.empty:
            print("âš ï¸ ç©ºã®ãƒãƒƒãƒï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        total_rows_loaded += len(df_batch)
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_batch):,} è¡Œ")

        X, y = parquet_handler.split(df_batch)
        print(f"ğŸ§  å­¦ç¿’å¯¾è±¡: {len(X):,} è¡Œï¼ˆç‰¹å¾´é‡ï¼‰, {len(y):,} è¡Œï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰")

        if X.empty or y.empty:
            print("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        trainer.train_incrementally(X, y)
        trainer.evaluate(X, y)
        total_rows_used += len(X)

    trainer.save_model(MODEL_SAVE_PATH)
    print(f"\nâœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODEL_SAVE_PATH}")
    print(f"ğŸ“ˆ å…¨ä½“ã§èª­ã¿è¾¼ã‚“ã è¡Œæ•°: {total_rows_loaded:,}")
    print(f"âœ… å­¦ç¿’ã«ä½¿ç”¨ã—ãŸè¡Œæ•°: {total_rows_used:,}")
